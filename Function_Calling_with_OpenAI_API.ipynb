{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/write-with-neurl/vellum/blob/main/Function_Calling_with_OpenAI_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e67f200",
      "metadata": {
        "id": "3e67f200"
      },
      "source": [
        "\n",
        "# üñ• Function Calling with OpenAI API\n",
        "\n",
        "This notebook covers how to:\n",
        "use the Chat Completions API in combination with external functions to extend:\n",
        "\n",
        "* Send a simple request to one of OpenAI's LLMs.\n",
        "* How to define functions for use by these GPT models.\n",
        "* How to generate parameters from GPT for functions.\n",
        "* How to call the function when GPT deems conditions are met.\n",
        "\n",
        "Before we begin, we can run the cell diectly below to wrap output text to a new line once the previous has been filled, which might be useful to view GPT output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yePxeWYoQbxY",
      "metadata": {
        "id": "yePxeWYoQbxY"
      },
      "outputs": [],
      "source": [
        "# Colab users only\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I4kiXDKjKzqT",
      "metadata": {
        "id": "I4kiXDKjKzqT"
      },
      "source": [
        "### üì¶ Installation\n",
        "\n",
        "Before we begin, let's install some dependencies for our demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e71f33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "80e71f33",
        "outputId": "df94f388-f3b0-43f8-e028-efc11954a00a",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet scipy tenacity tiktoken termcolor openai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TPrsCvLuK8r8",
      "metadata": {
        "id": "TPrsCvLuK8r8"
      },
      "source": [
        "Let's now import the necessary packages we need, declare what GPT model we would like to use, import our `api_key`, and instantiate an OpenAI client.\n",
        "\n",
        "You can select from a series of models listed [here](https://platform.openai.com/docs/models).\n",
        "\n",
        "Note: explciitly declaring the API key as a string in the notebook is a quick an easy way to use the key, but is not the best practice. Consider defining the `api_key` via an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab872c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dab872c5",
        "outputId": "48d3b5b4-2eca-4296-e2bd-20be55ec4049"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from termcolor import colored\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
        "API_KEY = \"USE_YOUR_API_KEY_HERE\"\n",
        "client = OpenAI(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K_xq1dDPLmMA",
      "metadata": {
        "id": "K_xq1dDPLmMA"
      },
      "source": [
        "## üì´ Sending Simple Requests\n",
        "\n",
        "The most basic example on interacting with the API is by sending in a message to talk to GPT.\n",
        "\n",
        "The main input is an array of message objects in the `messages` parameter, consisting of:\n",
        "\n",
        "* `role`: Can either be `system`, `user`, or `assistant`.\n",
        "    * The `assistant` is referencing the Assistant's API, which currently supports three types of tools: Code Interpreter, Retrieval, and Function Calling.\n",
        "* `content`: String input\n",
        "\n",
        "Conversations can be as short as one message or many back and forth turns.\n",
        "\n",
        "Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.\n",
        "\n",
        "To learn more about these requests, click [here](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q5jvph43Ndv4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "Q5jvph43Ndv4",
        "outputId": "989fcf5d-a97b-4f99-b9cb-e86c58df3bd0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine learning is a field of artificial intelligence (AI) that focuses on enabling computers to learn and improve from experience without being explicitly programmed. It involves the development of algorithms and models that allow machines to analyze and make predictions or decisions based on data patterns, rather than being explicitly programmed for a specific task. Through training on large sets of data, machine learning algorithms can identify and learn from patterns, and then apply this knowledge to make accurate predictions or take actions in new and unseen situations. Machine learning is used in various domains like image and speech recognition, natural language processing, recommendation systems, and many others.\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=GPT_MODEL,\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"What's machine learning?\"}\n",
        "  ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MgTyn5PjN9ZX",
      "metadata": {
        "id": "MgTyn5PjN9ZX"
      },
      "source": [
        "As mentioned, you can define what the system's role is by modifying the `role`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U39kuQzZL76W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "U39kuQzZL76W",
        "outputId": "8f6b6803-4b11-42fd-a29d-b83d136e1a79"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In code's clever dance,\n",
            "Machine learns from each nuance,\n",
            "Wisdom amplified.\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=GPT_MODEL,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic haiku assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's machine learning?\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MuhJBlQnPrvi",
      "metadata": {
        "id": "MuhJBlQnPrvi"
      },
      "source": [
        "Instead of waiting for responses to be completely generated, they can also be streamed while being generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HtQgkeJCPrDa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "HtQgkeJCPrDa",
        "outputId": "71f50296-6560-4d76-d8c2-67f44244c1d4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions without being explicitly programmed. It is based on the idea that systems can learn from data, identify patterns, and make predictions or decisions with minimal human intervention. Machine learning algorithms analyze large amounts of data to find patterns, improve performance, and make accurate predictions by continuously adapting and learning from new data. It is used in various applications such as image and speech recognition, autonomous vehicles, recommendation systems, fraud detection, and many more."
          ]
        }
      ],
      "source": [
        "stream = client.chat.completions.create(\n",
        "    model=GPT_MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mf97ZvjkQ34j",
      "metadata": {
        "id": "Mf97ZvjkQ34j"
      },
      "source": [
        "On top of this, if we have a `messages` list to keep track of our current conversation, we can hold a back and forth conversation with GPT.\n",
        "\n",
        "We will define a function to request a response to keep our sample clear and straightforward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J5GxTGZnXmUO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "J5GxTGZnXmUO",
        "outputId": "91d87a19-3137-4cf2-c930-3deeca1f4f48"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            tool_choice=tool_choice,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e\n",
        "\n",
        "\n",
        "def create_message(role, content):\n",
        "    return {'role': role, 'content': content}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K1MD2WenXoMP",
      "metadata": {
        "id": "K1MD2WenXoMP"
      },
      "source": [
        "Now, we can start a sample back and forth conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zLhazQcdQ2bD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "zLhazQcdQ2bD",
        "outputId": "b56fbc37-dad2-4dd9-fe56-1fb78af3d9e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User:\n",
            " What is machine learning? \n",
            "\n",
            "Response from GPT:\n",
            " Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. It involves the use of statistical techniques to enable machines to automatically learn and improve from experience or data, without being explicitly programmed for every specific task. By using patterns and inference, machine learning algorithms can identify and understand complex patterns and relationships within data, and make predictions or decisions based on those patterns. Machine learning finds applications in various industries, including areas like healthcare, finance, marketing, and many more. \n",
            "\n",
            "User:\n",
            " Can you summarize what you just told me in one sentence? \n",
            "\n",
            "Response from GPT:\n",
            " Machine learning is a branch of artificial intelligence that enables computers to learn from data, make predictions or decisions, and improve performance without being explicitly programmed. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's store our previous message from above.\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
        "]\n",
        "print('User:\\n', messages[0]['content'], '\\n')\n",
        "\n",
        "# Get a response, add it to our list of messages (our conversation)\n",
        "response = chat_completion_request(messages)\n",
        "message = response.choices[0].message.content\n",
        "print('Response from GPT:\\n', message, '\\n')\n",
        "messages.append(create_message('system', message))\n",
        "\n",
        "# Let's ask GPT to summarize its previous response.\n",
        "# We'll add our request to the messages list before asking for another response.\n",
        "user_req = create_message('user', 'Can you summarize what you just told me in one sentence?')\n",
        "messages.append(user_req)\n",
        "print('User:\\n', user_req['content'], '\\n')\n",
        "\n",
        "# Send the response, print the output.\n",
        "response = chat_completion_request(messages)\n",
        "print('Response from GPT:\\n', response.choices[0].message.content, '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22MAOhHNZAl9",
      "metadata": {
        "id": "22MAOhHNZAl9"
      },
      "source": [
        "## üîî Function Calling\n",
        "\n",
        "Function calling is a part of OpenAI's Assistants API. As mentioned previously, there are types of assistants at this current time of writing: Code Interpreter, Retrieval, and Function calling.\n",
        "\n",
        "Of course, in this demo, we'll focus on Function Calling.\n",
        "* Function calling is the ability of the LLM to perform a specific task by returning a deterministic and structured output.\n",
        "\n",
        "* In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions (depending on if the model supports multiple parallel calls, such a s GPT4).\n",
        "\n",
        "* The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.\n",
        "\n",
        "For more information on the Assistants API, click [here](https://platform.openai.com/docs/assistants/overview?context=with-streaming)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Itjf9yEpqv2k",
      "metadata": {
        "id": "Itjf9yEpqv2k"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We'll establish one more helper function to print out all of our messages now. For the demo below, we also require `chat_completion_request()` and `create_message()` from our `Sending Simple Requests` examples above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d1c99f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "c4d1c99f",
        "outputId": "1126b9a7-b557-46c7-b39d-e52dcea89d82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def pretty_print_conversation(messages):\n",
        "    role_to_color = {\n",
        "        \"system\": \"red\",\n",
        "        \"user\": \"green\",\n",
        "        \"assistant\": \"blue\",\n",
        "        \"function\": \"magenta\",\n",
        "    }\n",
        "\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"function\":\n",
        "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64c85e26",
      "metadata": {
        "id": "64c85e26"
      },
      "source": [
        "### ‚è≥ Generating Function Arguments\n",
        "\n",
        "Before generating our function arguments, we need to define what `tools` our OpenAI `client()` has access to. `tools` are a list of `dict` items that indicate what type of tool it is (Code Interpreter, Retrieval, and Function Calling), as well as other information describing that tool.\n",
        "\n",
        "Below, we initialize `function` tools for Function Calling. However, you can have any mix of `tools` that you deem fit.\n",
        "\n",
        "We initialize `get_current_weather()` and `get_n_day_weather_forecast()` with its necessary parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e25069",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d2e25069",
        "outputId": "72e87fac-afb8-433e-e55c-ecde9c1c1388"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"format\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\", \"format\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_n_day_weather_forecast\",\n",
        "            \"description\": \"Get an N-day weather forecast\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"format\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                    },\n",
        "                    \"num_days\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"The number of days to forecast\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\", \"format\", \"num_days\"]\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bRQqJEa4sdw_",
      "metadata": {
        "id": "bRQqJEa4sdw_"
      },
      "source": [
        "Now, we can begin sending requests to GPT with our `tools`.\n",
        "\n",
        "If we prompt the model about the current weather, it will respond with some clarifying questions (as shown in `content` in the response)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518d6827",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "518d6827",
        "outputId": "fa2e75b4-fa2f-449c-af12-759eb8416652"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sure, could you please provide me with your current location?', role='assistant', function_call=None, tool_calls=None))\n"
          ]
        }
      ],
      "source": [
        "# Define messages\n",
        "messages = []\n",
        "messages.append(\n",
        "    create_message(\n",
        "        \"system\",\n",
        "        \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
        "    )\n",
        ")\n",
        "messages.append(create_message(\"user\", \"What's the weather like today?\"))\n",
        "\n",
        "# Submit response\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "messages.append(chat_response.choices[0].message)\n",
        "print(chat_response.choices[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c999375",
      "metadata": {
        "id": "4c999375"
      },
      "source": [
        "Once we provide the missing information, it will generate the appropriate function arguments for us.\n",
        "\n",
        "Notice how the `finish_reason` is now `tool_calls`, indicating some `tool` defined in our `tools` schema is being called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c42a6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "23c42a6e",
        "outputId": "9cf4fa7d-4930-4258-db4d-a606f2004c43"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_npQlZt0Ef84rYiT6Dat8V1xO', function=Function(arguments='{\\n  \"location\": \"San Francisco, CA\",\\n  \"format\": \"celsius\"\\n}', name='get_current_weather'), type='function')]))\n"
          ]
        }
      ],
      "source": [
        "# Define messages\n",
        "messages.append(create_message(\"user\", \"I'm in San Francisco, CA\"))\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "\n",
        "# Submit response\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "messages.append(chat_response.choices[0].message)\n",
        "print(chat_response.choices[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14d4762",
      "metadata": {
        "id": "c14d4762"
      },
      "source": [
        "By prompting it differently, we can get it to target the other function we've told it about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa232e54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "fa232e54",
        "outputId": "7768f068-3303-4c55-fc73-65ae323ea54e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sure, I can help you with that. Please provide me with the number of days you would like to forecast for San Francisco, CA.', role='assistant', function_call=None, tool_calls=None))\n"
          ]
        }
      ],
      "source": [
        "# Define messages\n",
        "messages = []\n",
        "messages.append(\n",
        "    create_message(\n",
        "        \"system\",\n",
        "        \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
        "    )\n",
        ")\n",
        "messages.append(create_message(\"user\", \"what is the weather going to be like in San Francisco, CA over the next x days\"))\n",
        "\n",
        "# Submit response\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "messages.append(chat_response.choices[0].message)\n",
        "print(chat_response.choices[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6172ddac",
      "metadata": {
        "id": "6172ddac"
      },
      "source": [
        "Once again, the model is asking us for clarification because it doesn't have enough information yet. In this case it already knows the location for the forecast, but it needs to know how many days are required in the forecast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d8a543",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "c7d8a543",
        "outputId": "a9479d7b-814e-49ee-ebce-67a258936b99"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_WBBLCK4M3NL9wlR81bhqM2KB', function=Function(arguments='{\\n  \"location\": \"San Francisco, CA\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 5\\n}', name='get_n_day_weather_forecast'), type='function')]))\n"
          ]
        }
      ],
      "source": [
        "messages.append(create_message(\"user\", \"5 days\"))\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "print(chat_response.choices[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b758a0a",
      "metadata": {
        "id": "4b758a0a"
      },
      "source": [
        "#### Forcing the use of specific functions or no function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412f79ba",
      "metadata": {
        "id": "412f79ba"
      },
      "source": [
        "We can force the model to use a specific function, for example get_n_day_weather_forecast by using the function_call argument. By doing so, we force the model to make assumptions about how to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "559371b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "559371b7",
        "outputId": "33187cfc-50f4-44fb-a0df-6262907b63e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_FQSl6U46sIG2HyXHQ1UnNMrm', function=Function(arguments='{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 1\\n}', name='get_n_day_weather_forecast'), type='function')]))\n"
          ]
        }
      ],
      "source": [
        "# in this cell we force the model to use get_n_day_weather_forecast\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_n_day_weather_forecast\"}}\n",
        ")\n",
        "print(chat_response.choices[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7ab0f58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "a7ab0f58",
        "outputId": "f37619e5-ffbd-484b-c266-2078c619b1dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_I8NDXwfjmw3QmclV2pfZgBTZ', function=Function(arguments='{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\"\\n}', name='get_current_weather'), type='function')]))\n"
          ]
        }
      ],
      "source": [
        "# if we don't force the model to use get_n_day_weather_forecast it may not\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "print(chat_response.choices[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd70e48",
      "metadata": {
        "id": "3bd70e48"
      },
      "source": [
        "We can also force the model to not use a function at all. By doing so we prevent it from producing a proper function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acfe54e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "acfe54e6",
        "outputId": "61ab300c-7089-44d8-f21a-49598df133cc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{ \"location\": \"Toronto, Canada\", \"format\": \"celsius\" }', role='assistant', function_call=None, tool_calls=None))\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools, tool_choice=\"none\"\n",
        ")\n",
        "print(chat_response.choices[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b616353b",
      "metadata": {
        "id": "b616353b"
      },
      "source": [
        "#### Parallel Function Calling\n",
        "\n",
        "Newer models like gpt-4-1106-preview or gpt-3.5-turbo-1106 can call multiple functions in one turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380eeb68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "380eeb68",
        "outputId": "6bb65bec-b886-4173-d171-02b7b17c79b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ChatCompletionMessageToolCall(id='call_oEWfcqY5wiBNAGw8Rb6xlymf', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"format\": \"celsius\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function'), ChatCompletionMessageToolCall(id='call_yBIdc8jb2m4c3Z2zB4NUEofO', function=Function(arguments='{\"location\": \"Glasgow\", \"format\": \"celsius\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function')]\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in San Francisco and Glasgow over the next 4 days\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools, model='gpt-3.5-turbo-1106'\n",
        ")\n",
        "\n",
        "assistant_message = chat_response.choices[0].message.tool_calls\n",
        "print(assistant_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4482aee",
      "metadata": {
        "id": "b4482aee"
      },
      "source": [
        "### üìû Calling Functions from Generated Inputs\n",
        "Next, we'll implement some of these functions to return some response, showing how we'd \"link\" input from our LLM output to our code logic we'd write in a traditional development environment.\n",
        "\n",
        "For our \"linking\", we'll have a function that will check for the string of our returns output prompt from the LLM, and call the appropriate function based on that message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9sCKcHi9JJ_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "D9sCKcHi9JJ_",
        "outputId": "e90f55cd-3a26-47f6-ef46-cdacdcfa9d7b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def get_current_weather(location, format):\n",
        "    return \"Call successful from get_current_weather().\"\n",
        "\n",
        "\n",
        "def get_n_day_weather_forecast(location, format, num_days):\n",
        "    return \"Call successful from get_n_day_weather_forecast()\"\n",
        "\n",
        "\n",
        "def execute_function_call(message):\n",
        "    args = json.loads(msg.tool_calls[0].function.arguments)\n",
        "    if message.tool_calls[0].function.name == \"get_current_weather\":\n",
        "        results = get_current_weather(args[\"location\"], args[\"format\"])\n",
        "    elif message.tool_calls[0].function.name == \"get_n_day_weather_forecast\":\n",
        "        results = get_n_day_weather_forecast(args[\"location\"], args[\"format\"], args[\"num_days\"])\n",
        "    else:\n",
        "        results = f\"Error: function {message.tool_calls[0].function.name} does not exist\"\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u0Ry3_6rBu2y",
      "metadata": {
        "id": "u0Ry3_6rBu2y"
      },
      "source": [
        "Now, let's try to generate some our function arguments once more, but pipe this to call `execute_function_call()`, which will call other functions we've implemented based on the function name in `message`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g-mAtexXB_wQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "g-mAtexXB_wQ",
        "outputId": "5106e58e-dca3-44b0-e8c4-f100640100d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system: Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
            "\n",
            "user: what is the weather going to be like in San Francisco, CA?\n",
            "\n",
            "assistant: Function(arguments='{\\n  \"location\": \"San Francisco, CA\",\\n  \"format\": \"celsius\"\\n}', name='get_current_weather')\n",
            "\n",
            "function (get_current_weather): Call successful from get_current_weather().\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define messages\n",
        "messages = []\n",
        "messages.append(\n",
        "    create_message(\n",
        "        \"system\",\n",
        "        \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
        "    )\n",
        ")\n",
        "messages.append(create_message(\"user\", \"what is the weather going to be like in San Francisco, CA?\"))\n",
        "\n",
        "# Submit response\n",
        "chat_response = chat_completion_request(messages, tools)\n",
        "\n",
        "# Parse response\n",
        "msg = chat_response.choices[0].message\n",
        "messages.append({\"role\": msg.role, \"content\": msg.tool_calls[0].function})\n",
        "msg_func = str(msg.tool_calls[0].function)\n",
        "\n",
        "# Call corresponding function\n",
        "if msg.tool_calls:\n",
        "    results = execute_function_call(msg)\n",
        "    messages.append({\"role\": \"function\",\n",
        "                     \"tool_call_id\": msg.tool_calls[0].id,\n",
        "                     \"name\": msg.tool_calls[0].function.name,\n",
        "                     \"content\": results\n",
        "                     })\n",
        "pretty_print_conversation(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7znenERPRiM",
      "metadata": {
        "id": "a7znenERPRiM"
      },
      "source": [
        "# üéâ Congrats!\n",
        "\n",
        "With the demo above, you should be able to start development with LLMs incorporated into your application.\n",
        "\n",
        "To learn how to attach knowledge bases for knowledge retrieval, click [here](https://cookbook.openai.com/examples/how_to_call_functions_for_knowledge_retrieval)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
